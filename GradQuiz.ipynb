{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATASET_NAME = \"general_filtered\"\n",
    "# DATASET_NAME = \"science-technology_filtered\"\n",
    "# DATASET_NAME = \"history_filtered\"\n",
    "\n",
    "if not os.path.exists('results/' + DATASET_NAME):\n",
    "    os.makedirs('results/' + DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "file_path = \"\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df['choices'] = df['choices'].apply(ast.literal_eval)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/' + DATASET_NAME):\n",
    "    os.makedirs('results/' + DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_substrings(lst):\n",
    "    lst = sorted(lst, key=len, reverse=True)  \n",
    "    risultato = []\n",
    "\n",
    "    for i, parola in enumerate(lst):\n",
    "        is_sub = False\n",
    "        for j, altro in enumerate(lst):\n",
    "            if i != j and parola in altro:\n",
    "                is_sub = True\n",
    "                break\n",
    "        if not is_sub:\n",
    "            risultato.append(parola)\n",
    "    \n",
    "    return risultato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_strongest_entities(sentence):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    l = []\n",
    "    \n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    if len(entities) != 0:\n",
    "        for (e, label) in entities:\n",
    "            if label not in {'MONEY', 'DATE'} and e not in l:\n",
    "                l.append(e)\n",
    "    \n",
    "    keywords1 = [token.text for token in doc if token.pos_ in {\"PROPN\"} and not token.is_stop]\n",
    "    keywords2 = [token.text for token in doc if token.pos_ in {\"ADJ\"} and not token.is_stop]\n",
    "    keywords3 = [token.text for token in doc if token.pos_ in {\"NOUN\"} and not token.is_stop]\n",
    "    \n",
    "    keywords = keywords1 + keywords2 + keywords3\n",
    "    \n",
    "    if len(keywords) != 0:\n",
    "       for k in keywords:\n",
    "           if k not in l:\n",
    "               l.append(k)\n",
    "\n",
    "    l = remove_substrings(l)\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "\n",
    "class AdversarialAttackMistral:\n",
    "    def __init__(self, model, tokenizer, k=1, alpha=0.001, top_k=15):\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.top_k = top_k\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = torch.device(\"cuda:1\")\n",
    "\n",
    "    def perturb(self, question, entities):\n",
    "        if len(entities) == 0:\n",
    "            return [question]\n",
    "\n",
    "        input_tokens = self.tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        input_ids = input_tokens[\"input_ids\"].to(self.device)\n",
    "        vocab_embeds = self.model.get_input_embeddings().weight.to(self.device)\n",
    "\n",
    "        entity_infos = []\n",
    "        for e in entities:\n",
    "            tok = self.tokenizer(e, return_tensors=\"pt\")\n",
    "            tokens = torch.tensor([tok[\"input_ids\"][0].tolist()[1:]]).to(self.device)\n",
    "\n",
    "            keyword_positions = self.find_sequence_from_end(input_ids[0], tokens[0].tolist())\n",
    "            if not keyword_positions:\n",
    "                continue\n",
    "\n",
    "            original_embeddings = self.model.get_input_embeddings()(tokens).detach().clone()\n",
    "            similarity = torch.mm(original_embeddings.squeeze(0), vocab_embeds.T)\n",
    "\n",
    "            initial_alternatives = []\n",
    "            for pos in range(len(tokens[0])):\n",
    "                top_indices = torch.topk(similarity[pos], self.top_k + 1)[1].tolist()\n",
    "                top_alts = [idx for idx in top_indices if idx != tokens[0][pos].item()][:self.top_k]\n",
    "                initial_alternatives.append(top_alts)\n",
    "\n",
    "            entity_infos.append({\n",
    "                \"tokens\": tokens,\n",
    "                \"positions\": keyword_positions,\n",
    "                \"initial_alternatives\": initial_alternatives,\n",
    "            })\n",
    "\n",
    "        perturbed_questions = []\n",
    "        for variant_idx in range(self.top_k):\n",
    "            new_input_ids = input_ids.clone()\n",
    "\n",
    "            for entity in entity_infos:\n",
    "                tokens = entity[\"tokens\"].clone()\n",
    "                positions = entity[\"positions\"]\n",
    "                alternatives = entity[\"initial_alternatives\"]\n",
    "\n",
    "                perturbed_ids = tokens.clone()\n",
    "                for pos in range(len(tokens[0])):\n",
    "                    if variant_idx < len(alternatives[pos]):\n",
    "                        perturbed_ids[0, pos] = alternatives[pos][variant_idx]\n",
    "\n",
    "                initial_logits = self.model(inputs_embeds=self.model.get_input_embeddings()(perturbed_ids)).logits.detach()\n",
    "\n",
    "                for step in range(self.k):\n",
    "                    perturbed_embeddings = self.model.get_input_embeddings()(perturbed_ids).detach().clone()\n",
    "                    perturbed_embeddings.requires_grad_()\n",
    "\n",
    "                    outputs = self.model(inputs_embeds=perturbed_embeddings)\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    loss = torch.nn.functional.kl_div(\n",
    "                        torch.softmax(initial_logits, dim=-1).log(),\n",
    "                        torch.softmax(logits, dim=-1),\n",
    "                        reduction='batchmean'\n",
    "                    )\n",
    "\n",
    "                    grads = torch.autograd.grad(loss, perturbed_embeddings, retain_graph=True)[0]\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        perturbed_embeddings = perturbed_embeddings + self.alpha * grads.sign()\n",
    "                        similarity = torch.mm(perturbed_embeddings.squeeze(0), vocab_embeds.T)\n",
    "\n",
    "                        for pos in range(len(tokens[0])):\n",
    "                            top_indices = torch.topk(similarity[pos], self.top_k + 1)[1].tolist()\n",
    "                            for new_token_id in top_indices:\n",
    "                                if new_token_id != perturbed_ids[0, pos].item():\n",
    "                                    perturbed_ids[0, pos] = new_token_id\n",
    "                                    break\n",
    "\n",
    "                for i_pos, r_pos in enumerate(positions):\n",
    "                    new_input_ids[0][r_pos] = perturbed_ids[0][i_pos]\n",
    "\n",
    "            perturbed_text = self.tokenizer.decode(new_input_ids[0], skip_special_tokens=True)\n",
    "            perturbed_questions.append(perturbed_text)\n",
    "\n",
    "        return perturbed_questions\n",
    "\n",
    "\n",
    "    def find_sequence_from_end(self, v1, v2):\n",
    "        len_v1, len_v2 = len(v1), len(v2)\n",
    "        for i in range(len_v1 - len_v2, -1, -1):\n",
    "            if list(v1[i:i + len_v2]) == v2:\n",
    "                return list(range(i, i + len_v2))\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbation_step(row, entities):\n",
    "    d = {}\n",
    "    sentence = row['question']\n",
    "    d['question'] = sentence\n",
    "\n",
    "    perturbed_sentences = adversary.perturb(sentence, entities)\n",
    "    \n",
    "    quest = sentence.lower().replace(' ', '')\n",
    "    new_l = []\n",
    "    for i, x in enumerate(perturbed_sentences):\n",
    "        if x.lower().replace(' ', '') != quest:\n",
    "            new_l.append(x)\n",
    "    if len(new_l) > 0:\n",
    "        d['perturbed'] = perturbed_sentences\n",
    "    else:\n",
    "        d['perturbed'] = []\n",
    "            \n",
    "    d['real_choices'] = row['choices']\n",
    "    d['real_answer'] = row['answer']\n",
    "    \n",
    "    l = []\n",
    "    for question in d['perturbed']:\n",
    "        if question.lower().replace(' ', '') != d['question'].lower().replace(' ', ''):\n",
    "            l.append(question)\n",
    "    \n",
    "    d['perturbed'] = l\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refinement_step(d):\n",
    "    questions = d['perturbed']\n",
    "    \n",
    "    for j in range(len(questions)):\n",
    "        \n",
    "        r : ChatResponse = chat(model='deepseek-r1:14b', messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': \"You are a chatbot that must modify a given question to ensure it is fully correct.\\\n",
    "                Given a question, change it so that it meets the following conditions:\\n\\n\\\n",
    "                - The question must be grammatically correct.\\n - The question must be logically and factually coherent.\\n\\n\\\n",
    "                If the question is already correct, repeat the question exactly as given.\\n\\nYour response must be strictly the modified or repeated question.\\n\\n\\\n",
    "                No additional words or explanations. Any deviation from this format is not acceptable.\"},\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f'Question: {questions[j]}'\n",
    "            }\n",
    "        ])\n",
    "        gen = r.message.content\n",
    "        d['perturbed'][j] = r.message.content.split('</think>')[-1].split(\"\\n\")[-1].split(\"</s>\")[-1].strip().lower()\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers_generation_step(d):\n",
    "\n",
    "    answers = []\n",
    "    \n",
    "    for q in d['perturbed']:\n",
    "        response : ChatResponse = chat(model='deepseek-r1:14b', messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': \"Answer concisely in the format: 'answer: [your response]'.\\\n",
    "                    Do not provide explanations, context, or preambles. Assume every question makes sense, even if incorrect. If unclear, give the closest plausible answer without corrections.\\\n",
    "                    Do not refer to or repeat any part of the question in your answer. Provide only the response, without restating or referencing the subject of the question. \\\n",
    "                    No context, no explanations, no full sentences. Just the answer.\"\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f'Question: {q}'\n",
    "            } \n",
    "        ])\n",
    "        gen = response.message.content.split('</think>')[-1].strip()\n",
    "        if 'answer: ' in gen:\n",
    "            answers.append(gen.split('answer: ')[-1].strip().lower())\n",
    "    \n",
    "    d['answers_perturbed'] = answers\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top3_response_probability(question, answer, max_words=3):\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    \n",
    "    truncated_answer = \" \".join(answer.split()[:max_words])  \n",
    "    \n",
    "    question_tokens = tokenizer(question, add_special_tokens=False)\n",
    "    question_tokens = question_tokens.to(device).input_ids\n",
    "    truncated_answer_tokens = tokenizer(truncated_answer, add_special_tokens=False)\n",
    "    truncated_answer_tokens = truncated_answer_tokens.to(device).input_ids\n",
    "\n",
    "    num_tokens_to_check = min(len(truncated_answer_tokens), 3)\n",
    "\n",
    "    input_text = f\"{question} {truncated_answer}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[:, :-1, :]  \n",
    "\n",
    "    start_idx = len(question_tokens)  \n",
    "\n",
    "    answer_logits = logits[:, start_idx-1 : start_idx + num_tokens_to_check - 1, :]\n",
    "\n",
    "    answer_probabilities = F.softmax(answer_logits, dim=-1)\n",
    "\n",
    "    max_prob = answer_probabilities.max().item()\n",
    "\n",
    "    return max_prob  \n",
    "\n",
    "def select_hardest_wrong_answers(question, correct_answer, wrong_real_answers, wrong_answers, top_n=3):\n",
    "    system = {'role': 'system', 'content': 'You are a chatbot that has to answer with only the letter correspondent to the correct answer.'}\n",
    "\n",
    "    correct_prob = get_top3_response_probability(question, correct_answer)\n",
    "    \n",
    "    wrong_probs = [(wa.lower(), get_top3_response_probability(question, wa.lower())) for wa in wrong_answers]\n",
    "    \n",
    "    wrong_probs.sort(key=lambda x: abs(x[1] - correct_prob))\n",
    "    \n",
    "    wrong_real_answers = [s.lower() for s in wrong_real_answers]\n",
    "    \n",
    "    hardest_wrong_answers = []\n",
    "    for wa in wrong_probs:\n",
    "        if wa not in wrong_real_answers:\n",
    "            w = wa[0].replace('</s>', '')\n",
    "\n",
    "            r : ChatResponse = chat(model='deepseek-r1:14b', messages=[\n",
    "                {\n",
    "                    'role': 'system',\n",
    "                    'content': \"You are a chatbot that can only answer with 'yes' or 'no'. Given a question, an answer to verify, and a set of alternative answers, \"\n",
    "                    \"you must provide a single response based on the following rules:\\n\\n\"\n",
    "                    \"- Respond 'yes' if the answer to verify is either a correct response to the given question, is present among the alternative answers (either exactly or as a semantically equivalent answer), \\\n",
    "                    or if the answer to verify expresses uncertainty, lack of information, or an inability to determine the correct answer (e.g., 'I don't know', 'I'm not sure', 'There is not enough information').\\n\"\n",
    "                    \"- Respond 'no' if the answer to verify is neither correct nor present among the alternative answers.\\n\\n\"\n",
    "                    \"Your response **must** be strictly formatted as:\\n\\n\"\n",
    "                    \"answer: yes/no\\n\\n\"\n",
    "                    \"No additional words, explanations, or variations are allowed. Any deviation from this format is not acceptable.\"\n",
    "                },\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': f'Question: {question} Answer to verify: {w} Alternative answers: {hardest_wrong_answers}'\n",
    "                }\n",
    "            ])\n",
    "            gen = str(r.message.content)\n",
    "\n",
    "            if 'answer: no' in gen.split('</think>')[-1].strip().lower():\n",
    "                hardest_wrong_answers.append(w)\n",
    "            if len(hardest_wrong_answers) == 3:\n",
    "                break\n",
    "        else:\n",
    "            hardest_wrong_answers.append(w)\n",
    "            if len(hardest_wrong_answers) == 3:\n",
    "                break\n",
    "    \n",
    "    if len(hardest_wrong_answers) >= 1:\n",
    "        return hardest_wrong_answers\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def answers_choice_step(d):\n",
    "    choices_perturbed = []\n",
    "    s = set()\n",
    "    for ap in d['total_answers']:\n",
    "        x = ap.replace('</s>', '').replace('.', '').strip().lower()\n",
    "        if x not in s and x != d['real_choices'][d['real_answer']].replace('</s>', '').replace('.', '').strip().lower():\n",
    "            choices_perturbed.append(ap)\n",
    "        s.add(x)\n",
    "    if len(choices_perturbed) == 0:\n",
    "        d['choices_perturbed'] = None\n",
    "    else:\n",
    "        cp = select_hardest_wrong_answers(d['question'], d['real_choices'][d['real_answer']], d['wrong_real_answers'], choices_perturbed, 3)\n",
    "        if len(cp) >= 3:\n",
    "            d['choices_perturbed'] = cp\n",
    "        else:\n",
    "            wrong_old_alternatives = list(d['real_choices'])\n",
    "            wrong_old_alternatives.remove(d['real_choices'][d['real_answer']])\n",
    "            alt_prob = {}\n",
    "            for alt in wrong_old_alternatives:\n",
    "                alt_prob[alt.lower()] = get_top3_response_probability(d['question'], alt.lower())\n",
    "            top_alt = heapq.nlargest(3-len(list(cp)), alt_prob, key=alt_prob.get)\n",
    "            d['choices_perturbed'] = cp + top_alt\n",
    "                \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_step(q):\n",
    "    system = {'role': 'system', 'content': 'You are a chatbot that has to answer with only the letter correspondent to the correct answer.'}\n",
    "\n",
    "    if q['choices_perturbed'] != None and len(q['choices_perturbed']) == 3:\n",
    "        new_prompt = []\n",
    "        user = {'role': 'user'}\n",
    "        content = q['question']\n",
    "        \n",
    "        new_answers = []\n",
    "        new_answers.append((q['real_choices'][q['real_answer']].replace('</s>', '').replace('\"', '').strip().lower(), 'r'))\n",
    "        new_answers.append((q['choices_perturbed'][0].replace('</s>', '').replace('\"', '').strip(), 'p'))\n",
    "        new_answers.append((q['choices_perturbed'][1].replace('</s>', '').replace('\"', '').strip(), 'p'))\n",
    "        new_answers.append((q['choices_perturbed'][2].replace('</s>', '').replace('\"', '').strip(), 'p'))\n",
    "        random.shuffle(new_answers)\n",
    "        \n",
    "        user['content'] = content + ' A) ' + new_answers[0][0].lower() + '; B) ' + new_answers[1][0].lower() + '; C) ' + new_answers[2][0].lower() + '; D) ' + new_answers[3][0].lower() + '.'\n",
    "        new_prompt.append(system)\n",
    "        new_prompt.append(user)\n",
    "        \n",
    "        gt = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}\n",
    "        real = -1\n",
    "        for i, a in enumerate(new_answers):\n",
    "            if a[1] == 'r':\n",
    "                real = i\n",
    "                break\n",
    "        if real != -1:\n",
    "            new_ground_truth = gt[real]\n",
    "            \n",
    "            new_inputs = tokenizer.apply_chat_template(new_prompt, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "            new_inputs = {k: v for k, v in new_inputs.items()}\n",
    "            \n",
    "            new_out = model.generate(**new_inputs, max_new_tokens=1, do_sample=True).to(\"cuda:1\")\n",
    "            new_gen = tokenizer.decode(new_out[0][len(new_inputs[\"input_ids\"][0]):])\n",
    "\n",
    "        res = {'new_prompt': new_prompt, 'new_ground_truth': new_ground_truth, 'new_gen': new_gen, 'question': q['question']}\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy\n",
    "\n",
    "def generate_all_combinations(entities):\n",
    "    all_combinations = []\n",
    "    \n",
    "    for i in range(1, len(entities) + 1):\n",
    "        all_combinations.extend([list(c) for c in combinations(entities, i)])\n",
    "    \n",
    "    return all_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", device_map=\"cuda:1\", cache_dir='../models')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "for k_value in [1, 2, 3]:\n",
    "    for alpha_value in [0.01, 0.001, 0.0001]:        \n",
    "        \n",
    "        adversary = AdversarialAttackMistral(model, tokenizer, k=k_value, alpha=alpha_value, top_k=15)\n",
    "        \n",
    "        l = []\n",
    "        results = []\n",
    "        start_index = 0\n",
    "\n",
    "        results_file = f'results/new_results/{DATASET_NAME}/{DATASET_NAME}_final_results_circular_k{k_value}_alpha{alpha_value}.pkl'\n",
    "        index_file = f'results/new_results/{DATASET_NAME}/{DATASET_NAME}_index_k{k_value}_alpha{alpha_value}.pkl'\n",
    "        error_file = f'results/new_results/{DATASET_NAME}/{DATASET_NAME}_error_k{k_value}_alpha{alpha_value}.pkl'\n",
    "\n",
    "        if os.path.exists(results_file):\n",
    "            with open(results_file, 'rb') as f:\n",
    "                try:\n",
    "                    results = pickle.load(f)\n",
    "                except EOFError:\n",
    "                    results = []  \n",
    "\n",
    "        if os.path.exists(index_file):\n",
    "            with open(index_file, 'rb') as f:\n",
    "                try:\n",
    "                    start_index = pickle.load(f)\n",
    "                except EOFError:\n",
    "                    start_index = 0\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            if index <= start_index or index==0:\n",
    "                continue  \n",
    "\n",
    "            question = row['question']\n",
    "            wrong = False\n",
    "            it = 0\n",
    "\n",
    "            entities = find_strongest_entities(question)\n",
    "            entities_combinations = generate_all_combinations(entities)\n",
    "\n",
    "            while not wrong and it < len(entities_combinations):\n",
    "                try:\n",
    "                    result = None\n",
    "                    start = timeit.default_timer()\n",
    "                    first_op = start \n",
    "                    d = perturbation_step(row, entities_combinations[it])\n",
    "                    stop = timeit.default_timer()\n",
    "\n",
    "                    start = timeit.default_timer()\n",
    "                    d = refinement_step(d)\n",
    "                    stop = timeit.default_timer()\n",
    "\n",
    "                    start = timeit.default_timer()\n",
    "                    d = answers_generation_step(d)\n",
    "                    wrong_real_answers = [d['real_choices'][i] for i in range(len(d['real_choices'])) if i != d['real_answer']]\n",
    "                    d['wrong_real_answers'] = wrong_real_answers.copy()\n",
    "                    d['total_answers'] = list(set(d['answers_perturbed'] + wrong_real_answers))\n",
    "                    stop = timeit.default_timer()\n",
    "\n",
    "                    start = timeit.default_timer()\n",
    "                    d = answers_choice_step(d)\n",
    "                    stop = timeit.default_timer()\n",
    "                    last_op = stop\n",
    "                    if d['choices_perturbed'] != None:\n",
    "                        if set(d['choices_perturbed']) == set(wrong_real_answers):\n",
    "                            it += 1\n",
    "                            continue\n",
    "                    \n",
    "                        start = timeit.default_timer()\n",
    "                        result = results_step(d)\n",
    "                        stop = timeit.default_timer()\n",
    "\n",
    "                        if result and result['new_gen'] != result['new_ground_truth']:\n",
    "                            wrong = True\n",
    "                            results.append(result)\n",
    "\n",
    "                            with open(results_file, 'ab') as f:\n",
    "                                pickle.dump([result], f)  \n",
    "                            with open(index_file, 'wb') as f:\n",
    "                                pickle.dump(index, f)\n",
    "                except Exception as e:\n",
    "                    if it == (len(entities_combinations)-1):\n",
    "                        with open(results_file, 'ab') as f:\n",
    "                            pickle.dump([e, index, it], f)\n",
    "                it += 1\n",
    "\n",
    "            if not wrong:\n",
    "                with open(results_file, 'ab') as f:\n",
    "                    pickle.dump([result], f)\n",
    "\n",
    "            with open(index_file, 'wb') as f:\n",
    "                pickle.dump(index, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_cuda_env",
   "language": "python",
   "name": "kernel_cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
